{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaEd0Ei5N5cPfPIw66OaVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmaNagirireddi/ML_7/blob/main/Ml_7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import DataLoader; from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "sab6EGBJ3ZB-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MBgdr6e39ND",
        "outputId": "d443af1d-967c-46c2-816f-d13eabe48eaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = torch.stack([img_t.view(3, -1) for img_t, _ in train_dataset], dim=1).mean(dim=1), torch.stack([img_t.view(3, -1) for img_t, _ in train_dataset], dim=1).std(dim=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og-rKEE24BYX",
        "outputId": "895bc8dc-f6d5-40bb-e049-992406d211fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 49237348.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNB4fFpK5_cO",
        "outputId": "d679b47f-73c8-4826-e594-5f88c8fbe683"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.5126, 0.5104, 0.5139,  ..., 0.4972, 0.4963, 0.4966],\n",
              "         [0.5336, 0.5311, 0.5343,  ..., 0.4929, 0.4926, 0.4935],\n",
              "         [0.5198, 0.5171, 0.5199,  ..., 0.4464, 0.4467, 0.4486]]),\n",
              " tensor([[0.2879, 0.2841, 0.2833,  ..., 0.2508, 0.2517, 0.2546],\n",
              "         [0.2859, 0.2820, 0.2811,  ..., 0.2415, 0.2425, 0.2457],\n",
              "         [0.3155, 0.3121, 0.3115,  ..., 0.2545, 0.2557, 0.2591]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = datasets.CIFAR10('./data', train=True, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]))\n"
      ],
      "metadata": {
        "id": "Okw0FgJx6YAg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10('./data', train=False, download=False,\n",
        "                               transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]))\n",
        "\n",
        "first_image, label = cifar10[0]\n",
        "print(first_image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZDM2SNo6qyE",
        "outputId": "a4dd467a-ff7d-498b-d1ac-ec1c92602fc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = [DataLoader(dataset, batch_size=32, shuffle=train, num_workers=2) for dataset, train in zip([cifar10, cifar10_val], [True, False])]\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
      ],
      "metadata": {
        "id": "TpXEJ2Cb67T6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1, self.act1, self.pool1 = nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.Tanh(), nn.MaxPool2d(2)\n",
        "        self.conv2, self.act2, self.pool2 = nn.Conv2d(16, 8, kernel_size=3, padding=1), nn.Tanh(), nn.MaxPool2d(2)\n",
        "        self.fc1, self.act3, self.fc2 = nn.Linear(8 * 8 * 8, 32), nn.Tanh(), nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool2(self.act2(self.conv2(self.pool1(self.act1(self.conv1(x))))))\n",
        "        x = x.view(-1, 8 * 8 * 8)\n",
        "        x = self.fc2(self.act3(self.fc1(x)))\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOPUO6e69zk",
        "outputId": "fdb684f3-4293-4af5-9479-220f78fae7e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.1 ms (started: 2023-12-12 21:38:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYyRY6xI-F0f",
        "outputId": "77caccc8-1645-4079-f72b-68945a20e604"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18354, [432, 16, 1152, 8, 16384, 32, 320, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 15.6 ms (started: 2023-12-12 21:39:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 2 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))\n",
        "\n"
      ],
      "metadata": {
        "id": "IfgEBV7r-nGS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(n_epochs = 300, optimizer = optimizer, model = model, loss_fn = loss_fn,train_loader = train_loader,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ30f_pF-IvZ",
        "outputId": "d8a089f6-c768-460e-b8fd-169de19e6989"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 21:58:37.014697 Epoch 1, Training loss 2.0162188311671967\n",
            "2023-12-12 21:59:18.921000 Epoch 2, Training loss 1.7634930206686639\n",
            "2023-12-12 22:00:45.500216 Epoch 4, Training loss 1.491888718684311\n",
            "2023-12-12 22:02:10.591949 Epoch 6, Training loss 1.3718824337815385\n",
            "2023-12-12 22:03:38.360362 Epoch 8, Training loss 1.2740474026221449\n",
            "2023-12-12 22:05:03.555960 Epoch 10, Training loss 1.1974983867019644\n",
            "2023-12-12 22:06:28.593377 Epoch 12, Training loss 1.1374762334177255\n",
            "2023-12-12 22:07:54.535589 Epoch 14, Training loss 1.0957224607620093\n",
            "2023-12-12 22:09:19.703955 Epoch 16, Training loss 1.0601110463709478\n",
            "2023-12-12 22:10:47.887607 Epoch 18, Training loss 1.0291698164952077\n",
            "2023-12-12 22:12:12.427987 Epoch 20, Training loss 1.0013239630652815\n",
            "2023-12-12 22:13:37.625106 Epoch 22, Training loss 0.9786256111948691\n",
            "2023-12-12 22:15:02.892831 Epoch 24, Training loss 0.9567744025336508\n",
            "2023-12-12 22:16:28.331397 Epoch 26, Training loss 0.9359314717600108\n",
            "2023-12-12 22:17:54.883259 Epoch 28, Training loss 0.9184819264027774\n",
            "2023-12-12 22:19:21.012543 Epoch 30, Training loss 0.9028092273665816\n",
            "2023-12-12 22:20:46.161848 Epoch 32, Training loss 0.8920406337894137\n",
            "2023-12-12 22:22:10.840070 Epoch 34, Training loss 0.8792822330504122\n",
            "2023-12-12 22:23:34.029621 Epoch 36, Training loss 0.8688108228204195\n",
            "2023-12-12 22:24:58.802008 Epoch 38, Training loss 0.8584700995851355\n",
            "2023-12-12 22:26:24.148433 Epoch 40, Training loss 0.8488024220899548\n",
            "2023-12-12 22:27:49.699417 Epoch 42, Training loss 0.8394111223004358\n",
            "2023-12-12 22:29:15.613763 Epoch 44, Training loss 0.8324615139028301\n",
            "2023-12-12 22:30:41.903700 Epoch 46, Training loss 0.8245368983282153\n",
            "2023-12-12 22:32:07.801963 Epoch 48, Training loss 0.8163411627949961\n",
            "2023-12-12 22:33:33.959168 Epoch 50, Training loss 0.809624812563362\n",
            "2023-12-12 22:34:59.922250 Epoch 52, Training loss 0.8026730525676552\n",
            "2023-12-12 22:36:26.287871 Epoch 54, Training loss 0.7965614641337748\n",
            "2023-12-12 22:37:53.617348 Epoch 56, Training loss 0.790628609449967\n",
            "2023-12-12 22:39:20.689564 Epoch 58, Training loss 0.7846103087258156\n",
            "2023-12-12 22:40:54.259922 Epoch 60, Training loss 0.777487051685143\n",
            "2023-12-12 22:42:33.708649 Epoch 62, Training loss 0.7739064302819464\n",
            "2023-12-12 22:43:56.294529 Epoch 64, Training loss 0.7685546997334342\n",
            "2023-12-12 22:45:18.105384 Epoch 66, Training loss 0.7628173040764411\n",
            "2023-12-12 22:46:41.113095 Epoch 68, Training loss 0.7583369986175577\n",
            "2023-12-12 22:48:02.766174 Epoch 70, Training loss 0.7539863725909797\n",
            "2023-12-12 22:49:26.228265 Epoch 72, Training loss 0.7477094018093461\n",
            "2023-12-12 22:50:48.099737 Epoch 74, Training loss 0.743983574268763\n",
            "2023-12-12 22:52:10.903842 Epoch 76, Training loss 0.7386120351988946\n",
            "2023-12-12 22:53:32.588187 Epoch 78, Training loss 0.7331918694860186\n",
            "2023-12-12 22:54:54.135737 Epoch 80, Training loss 0.7287420835870001\n",
            "2023-12-12 22:56:16.558648 Epoch 82, Training loss 0.7257299176643571\n",
            "2023-12-12 22:57:37.880185 Epoch 84, Training loss 0.7209475314068368\n",
            "2023-12-12 22:58:59.866006 Epoch 86, Training loss 0.716535467740215\n",
            "2023-12-12 23:00:21.579645 Epoch 88, Training loss 0.7128260667671633\n",
            "2023-12-12 23:01:44.243029 Epoch 90, Training loss 0.7082496086121215\n",
            "2023-12-12 23:03:07.446356 Epoch 92, Training loss 0.7058466395834828\n",
            "2023-12-12 23:04:29.249006 Epoch 94, Training loss 0.7021345088015432\n",
            "2023-12-12 23:05:51.634814 Epoch 96, Training loss 0.6989595403756632\n",
            "2023-12-12 23:07:13.027992 Epoch 98, Training loss 0.6952738764188479\n",
            "2023-12-12 23:08:34.214651 Epoch 100, Training loss 0.6903468443990668\n",
            "2023-12-12 23:09:56.250794 Epoch 102, Training loss 0.6881494521713623\n",
            "2023-12-12 23:11:18.246692 Epoch 104, Training loss 0.6838775881187386\n",
            "2023-12-12 23:12:40.246394 Epoch 106, Training loss 0.6809316820576977\n",
            "2023-12-12 23:14:02.500854 Epoch 108, Training loss 0.6784071546533833\n",
            "2023-12-12 23:15:27.065627 Epoch 110, Training loss 0.6753108048682932\n",
            "2023-12-12 23:16:52.195975 Epoch 112, Training loss 0.6724395460408666\n",
            "2023-12-12 23:18:16.675214 Epoch 114, Training loss 0.669726350804424\n",
            "2023-12-12 23:19:42.337206 Epoch 116, Training loss 0.6674269906928777\n",
            "2023-12-12 23:21:06.176714 Epoch 118, Training loss 0.6636792485366392\n",
            "2023-12-12 23:22:31.625471 Epoch 120, Training loss 0.6612877338514913\n",
            "2023-12-12 23:23:55.335574 Epoch 122, Training loss 0.6587425017981883\n",
            "2023-12-12 23:25:16.289544 Epoch 124, Training loss 0.6553580435112004\n",
            "2023-12-12 23:26:37.069932 Epoch 126, Training loss 0.653299470424957\n",
            "2023-12-12 23:27:58.937697 Epoch 128, Training loss 0.6506823256345051\n",
            "2023-12-12 23:29:19.939367 Epoch 130, Training loss 0.648258819032813\n",
            "2023-12-12 23:30:41.233078 Epoch 132, Training loss 0.6456230762593277\n",
            "2023-12-12 23:32:03.225647 Epoch 134, Training loss 0.6449719160566549\n",
            "2023-12-12 23:33:25.040454 Epoch 136, Training loss 0.6417486562067286\n",
            "2023-12-12 23:34:47.593623 Epoch 138, Training loss 0.6378574856483114\n",
            "2023-12-12 23:36:09.529211 Epoch 140, Training loss 0.6358249589152958\n",
            "2023-12-12 23:37:31.120087 Epoch 142, Training loss 0.6336021695448004\n",
            "2023-12-12 23:38:54.200857 Epoch 144, Training loss 0.6307020801336259\n",
            "2023-12-12 23:40:16.462041 Epoch 146, Training loss 0.6297578576885526\n",
            "2023-12-12 23:41:37.476678 Epoch 148, Training loss 0.6268225334718099\n",
            "2023-12-12 23:43:00.186220 Epoch 150, Training loss 0.6235182977012356\n",
            "2023-12-12 23:44:22.477990 Epoch 152, Training loss 0.6235646861593437\n",
            "2023-12-12 23:45:44.237362 Epoch 154, Training loss 0.6204440219094381\n",
            "2023-12-12 23:47:05.971421 Epoch 156, Training loss 0.6172538964111177\n",
            "2023-12-12 23:48:29.399378 Epoch 158, Training loss 0.6173722649093174\n",
            "2023-12-12 23:49:51.588224 Epoch 160, Training loss 0.6157309050145356\n",
            "2023-12-12 23:51:11.946624 Epoch 162, Training loss 0.6107900461272511\n",
            "2023-12-12 23:52:31.761184 Epoch 164, Training loss 0.6121916224432113\n",
            "2023-12-12 23:53:50.293993 Epoch 166, Training loss 0.608651474346895\n",
            "2023-12-12 23:55:09.168201 Epoch 168, Training loss 0.6078841625653264\n",
            "2023-12-12 23:56:28.464636 Epoch 170, Training loss 0.6050792198504329\n",
            "2023-12-12 23:57:48.592099 Epoch 172, Training loss 0.6034232612003756\n",
            "2023-12-12 23:59:10.997534 Epoch 174, Training loss 0.6016677999130601\n",
            "2023-12-13 00:00:32.878505 Epoch 176, Training loss 0.6000019897280446\n",
            "2023-12-13 00:01:55.161489 Epoch 178, Training loss 0.5986396341067751\n",
            "2023-12-13 00:03:15.368049 Epoch 180, Training loss 0.5978641051541814\n",
            "2023-12-13 00:04:35.674027 Epoch 182, Training loss 0.5942570311791452\n",
            "2023-12-13 00:05:58.870379 Epoch 184, Training loss 0.5930868219155485\n",
            "2023-12-13 00:07:22.346720 Epoch 186, Training loss 0.5928843557987067\n",
            "2023-12-13 00:08:45.408144 Epoch 188, Training loss 0.5916315555343847\n",
            "2023-12-13 00:10:07.654290 Epoch 190, Training loss 0.5894066008460491\n",
            "2023-12-13 00:11:29.026642 Epoch 192, Training loss 0.5871423865904284\n",
            "2023-12-13 00:12:47.427921 Epoch 194, Training loss 0.5866740738872982\n",
            "2023-12-13 00:14:06.441634 Epoch 196, Training loss 0.5869000377633687\n",
            "2023-12-13 00:15:26.220743 Epoch 198, Training loss 0.5826766930349038\n",
            "2023-12-13 00:16:45.982879 Epoch 200, Training loss 0.5814041025803217\n",
            "2023-12-13 00:18:06.880531 Epoch 202, Training loss 0.5787389691909561\n",
            "2023-12-13 00:19:29.867982 Epoch 204, Training loss 0.5797973079678348\n",
            "2023-12-13 00:20:51.523972 Epoch 206, Training loss 0.5773185443161698\n",
            "2023-12-13 00:22:13.260078 Epoch 208, Training loss 0.5760070111059472\n",
            "2023-12-13 00:23:35.384049 Epoch 210, Training loss 0.5728285946809423\n",
            "2023-12-13 00:24:58.801619 Epoch 212, Training loss 0.5725354157064272\n",
            "2023-12-13 00:26:20.081360 Epoch 214, Training loss 0.570395138676819\n",
            "2023-12-13 00:27:40.591446 Epoch 216, Training loss 0.571158683170443\n",
            "2023-12-13 00:29:01.819709 Epoch 218, Training loss 0.57019814174346\n",
            "2023-12-13 00:30:22.444037 Epoch 220, Training loss 0.5695483453590852\n",
            "2023-12-13 00:31:42.029397 Epoch 222, Training loss 0.5648110849625619\n",
            "2023-12-13 00:33:03.258182 Epoch 224, Training loss 0.5652586871095936\n",
            "2023-12-13 00:34:23.811349 Epoch 226, Training loss 0.5638047097741491\n",
            "2023-12-13 00:35:44.351674 Epoch 228, Training loss 0.5634297476247754\n",
            "2023-12-13 00:37:06.553637 Epoch 230, Training loss 0.561177105740513\n",
            "2023-12-13 00:38:28.043269 Epoch 232, Training loss 0.5606549025877662\n",
            "2023-12-13 00:39:49.099004 Epoch 234, Training loss 0.5606883001297026\n",
            "2023-12-13 00:41:11.029911 Epoch 236, Training loss 0.5578466066923897\n",
            "2023-12-13 00:42:31.275212 Epoch 238, Training loss 0.5570983175769486\n",
            "2023-12-13 00:43:52.882365 Epoch 240, Training loss 0.5553154606191094\n",
            "2023-12-13 00:45:16.705457 Epoch 242, Training loss 0.5533237047969838\n",
            "2023-12-13 00:46:38.997840 Epoch 244, Training loss 0.5539270981269724\n",
            "2023-12-13 00:48:00.970365 Epoch 246, Training loss 0.5516471216821914\n",
            "2023-12-13 00:49:21.809886 Epoch 248, Training loss 0.5511165294805755\n",
            "2023-12-13 00:50:43.580725 Epoch 250, Training loss 0.5497024292721773\n",
            "2023-12-13 00:52:05.632500 Epoch 252, Training loss 0.5509780661186294\n",
            "2023-12-13 00:53:26.713152 Epoch 254, Training loss 0.5483915014263919\n",
            "2023-12-13 00:54:46.465298 Epoch 256, Training loss 0.5472127307406471\n",
            "2023-12-13 00:56:07.523846 Epoch 258, Training loss 0.5452586868992242\n",
            "2023-12-13 00:57:26.633076 Epoch 260, Training loss 0.5457113151202726\n",
            "2023-12-13 00:58:45.721416 Epoch 262, Training loss 0.5454255575147431\n",
            "2023-12-13 01:00:04.122024 Epoch 264, Training loss 0.5426471927739165\n",
            "2023-12-13 01:01:22.872568 Epoch 266, Training loss 0.542301788868959\n",
            "2023-12-13 01:02:43.514384 Epoch 268, Training loss 0.5412974876477895\n",
            "2023-12-13 01:04:03.787991 Epoch 270, Training loss 0.539408939878654\n",
            "2023-12-13 01:05:22.591405 Epoch 272, Training loss 0.5395303781113356\n",
            "2023-12-13 01:06:41.109717 Epoch 274, Training loss 0.5389877076801437\n",
            "2023-12-13 01:08:00.129855 Epoch 276, Training loss 0.5394665750167559\n",
            "2023-12-13 01:09:19.599229 Epoch 278, Training loss 0.5347012475018611\n",
            "2023-12-13 01:10:38.842706 Epoch 280, Training loss 0.5346071849508054\n",
            "2023-12-13 01:11:57.469899 Epoch 282, Training loss 0.5334575412142307\n",
            "2023-12-13 01:13:15.288042 Epoch 284, Training loss 0.5331589358160868\n",
            "2023-12-13 01:14:33.716369 Epoch 286, Training loss 0.5321169532549656\n",
            "2023-12-13 01:15:53.049621 Epoch 288, Training loss 0.5305562874545222\n",
            "2023-12-13 01:17:13.375484 Epoch 290, Training loss 0.5295931714041459\n",
            "2023-12-13 01:18:33.504596 Epoch 292, Training loss 0.5291975272814636\n",
            "2023-12-13 01:19:52.803060 Epoch 294, Training loss 0.5293393999604923\n",
            "2023-12-13 01:21:12.633786 Epoch 296, Training loss 0.5295382876073003\n",
            "2023-12-13 01:22:32.676511 Epoch 298, Training loss 0.5279749385498064\n",
            "2023-12-13 01:23:53.839356 Epoch 300, Training loss 0.527983232837199\n",
            "time: 3h 25min 58s (started: 2023-12-12 21:57:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections"
      ],
      "metadata": {
        "id": "KgQMq0uF_EUQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "valLoader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "all_acc_dict = collections.OrderedDict()\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    accdict = {}\n",
        "    predictions, exp_labels = [], []\n",
        "\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs, labels = imgs.to(device=device), labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                correct += int((torch.max(outputs, 1)[1] == labels).sum())\n",
        "                total += labels.size(0)\n",
        "\n",
        "                predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "                exp_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(f\"Accuracy {name}: {correct / total:.2f}\")\n",
        "        accdict[name] = correct / total\n",
        "\n",
        "    return accdict, predictions, exp_labels"
      ],
      "metadata": {
        "id": "Df6clD2ZDdgU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, predictions, expected_labels = validate(model, trainLoader, valLoader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A4VzdvMLUrz",
        "outputId": "30a6aa75-fa10-496b-9468-30d28ac5a9d5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.79\n",
            "Accuracy val: 0.61\n",
            "time: 40.8 s (started: 2023-12-13 01:34:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
        "precision = precision_score(predictions, expected_labels, average='macro')\n",
        "recall = recall_score(predictions, expected_labels, average='macro')\n",
        "cnf_matrix = confusion_matrix(predictions, expected_labels)\n",
        "print(cnf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2u8g-wkLw4O",
        "outputId": "1df1222b-ff8d-4836-9bf9-3500dad7c865"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4755  169  259   93   85   43   40   60  360  211]\n",
            " [  81 5140   19   26   10   11   14   14  124  383]\n",
            " [ 243   32 3912  324  283  217  234  139   81   48]\n",
            " [ 124   64  288 3268  197  667  340  141   79   64]\n",
            " [ 149   47  550  368 4697  247  260  337   68   60]\n",
            " [  63   48  385 1322  226 4440  184  362   33   77]\n",
            " [  38   29  274  272  178   88 4845   18   45   27]\n",
            " [  96   39  210  234  283  251   38 4867   42  121]\n",
            " [ 356  159   88   51   28   15   29   23 5088  175]\n",
            " [  95  273   15   42   13   21   16   39   80 4834]]\n",
            "time: 585 ms (started: 2023-12-13 01:35:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(classification_report(predictions, expected_labels, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp6h_5aYL11T",
        "outputId": "a2e2fa1a-6af3-4b9f-dd85-15eb0e603c08"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.79      0.78      0.79      6075\n",
            "  automobile       0.86      0.88      0.87      5822\n",
            "        bird       0.65      0.71      0.68      5513\n",
            "         cat       0.54      0.62      0.58      5232\n",
            "        deer       0.78      0.69      0.73      6783\n",
            "         dog       0.74      0.62      0.68      7140\n",
            "        frog       0.81      0.83      0.82      5814\n",
            "       horse       0.81      0.79      0.80      6181\n",
            "        ship       0.85      0.85      0.85      6012\n",
            "       truck       0.81      0.89      0.85      5428\n",
            "\n",
            "    accuracy                           0.76     60000\n",
            "   macro avg       0.76      0.77      0.76     60000\n",
            "weighted avg       0.77      0.76      0.76     60000\n",
            "\n",
            "time: 267 ms (started: 2023-12-13 01:35:45 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 4, 32)\n",
        "        self.act4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = self.pool3(self.act3(self.conv3(out)))\n",
        "        out = out.view(-1, 4 * 4 * 4)\n",
        "        out = self.act4(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "19B87zdDL5JI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Net2().to(device)\n"
      ],
      "metadata": {
        "id": "qjDCjuSyMlrS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "sfkuXDKf5rBl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model2 = Net2().to(device=device)\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer2,\n",
        "    model = model2,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjit7rb_NLDE",
        "outputId": "88ae05cc-2c69-48e3-f15a-b4de748b8d7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-13 02:00:07.617124 Epoch 1, Training loss 2.212731592185662\n",
            "2023-12-13 02:00:22.718318 Epoch 2, Training loss 1.9845358686678856\n",
            "2023-12-13 02:00:53.246347 Epoch 4, Training loss 1.722589336240383\n",
            "2023-12-13 02:01:23.437815 Epoch 6, Training loss 1.5700147395853497\n",
            "2023-12-13 02:01:55.008355 Epoch 8, Training loss 1.4737615467947158\n",
            "2023-12-13 02:02:26.318886 Epoch 10, Training loss 1.3993406246995073\n",
            "2023-12-13 02:02:56.613258 Epoch 12, Training loss 1.3490596872461422\n",
            "2023-12-13 02:03:26.906569 Epoch 14, Training loss 1.3148994368055593\n",
            "2023-12-13 02:03:57.147142 Epoch 16, Training loss 1.2820321132459909\n",
            "2023-12-13 02:04:27.706434 Epoch 18, Training loss 1.253321915026516\n",
            "2023-12-13 02:04:58.728041 Epoch 20, Training loss 1.2270956954077992\n",
            "2023-12-13 02:05:28.750207 Epoch 22, Training loss 1.2034590320513987\n",
            "2023-12-13 02:05:58.742817 Epoch 24, Training loss 1.1855735565390428\n",
            "2023-12-13 02:06:28.715447 Epoch 26, Training loss 1.1677957881442116\n",
            "2023-12-13 02:06:58.521277 Epoch 28, Training loss 1.152568186792876\n",
            "2023-12-13 02:07:28.868663 Epoch 30, Training loss 1.137977940575851\n",
            "2023-12-13 02:07:59.785104 Epoch 32, Training loss 1.1281713855540967\n",
            "2023-12-13 02:08:29.891133 Epoch 34, Training loss 1.1162032981205474\n",
            "2023-12-13 02:08:59.984288 Epoch 36, Training loss 1.1060418872272266\n",
            "2023-12-13 02:09:30.005257 Epoch 38, Training loss 1.0980290106647765\n",
            "2023-12-13 02:10:00.516040 Epoch 40, Training loss 1.089980431834755\n",
            "2023-12-13 02:10:31.369294 Epoch 42, Training loss 1.080969400113196\n",
            "2023-12-13 02:11:01.297499 Epoch 44, Training loss 1.0739155903344264\n",
            "2023-12-13 02:11:31.448031 Epoch 46, Training loss 1.0676440385448964\n",
            "2023-12-13 02:12:01.366247 Epoch 48, Training loss 1.0612836273582391\n",
            "2023-12-13 02:12:31.351298 Epoch 50, Training loss 1.0556394661326542\n",
            "2023-12-13 02:13:02.154911 Epoch 52, Training loss 1.0497638699038865\n",
            "2023-12-13 02:13:32.417414 Epoch 54, Training loss 1.0436281315658404\n",
            "2023-12-13 02:14:02.369944 Epoch 56, Training loss 1.0392286134193012\n",
            "2023-12-13 02:14:32.405495 Epoch 58, Training loss 1.0351759331000736\n",
            "2023-12-13 02:15:02.378633 Epoch 60, Training loss 1.0299414166861482\n",
            "2023-12-13 02:15:33.166361 Epoch 62, Training loss 1.0251278458806254\n",
            "2023-12-13 02:16:03.846755 Epoch 64, Training loss 1.0215888941836784\n",
            "2023-12-13 02:16:33.814092 Epoch 66, Training loss 1.0169235557089071\n",
            "2023-12-13 02:17:03.872864 Epoch 68, Training loss 1.0148355409007548\n",
            "2023-12-13 02:17:33.872700 Epoch 70, Training loss 1.0094600095773292\n",
            "2023-12-13 02:18:03.825372 Epoch 72, Training loss 1.0057426452484277\n",
            "2023-12-13 02:18:34.608590 Epoch 74, Training loss 1.0027990525640795\n",
            "2023-12-13 02:19:05.116531 Epoch 76, Training loss 1.0003116153695089\n",
            "2023-12-13 02:19:35.147430 Epoch 78, Training loss 0.9963583322742101\n",
            "2023-12-13 02:20:05.078015 Epoch 80, Training loss 0.9949500767318794\n",
            "2023-12-13 02:20:35.232233 Epoch 82, Training loss 0.9918656431500564\n",
            "2023-12-13 02:21:07.810549 Epoch 84, Training loss 0.9878242818443367\n",
            "2023-12-13 02:21:38.102109 Epoch 86, Training loss 0.9877937045853461\n",
            "2023-12-13 02:22:07.876044 Epoch 88, Training loss 0.9843627271597343\n",
            "2023-12-13 02:22:38.135210 Epoch 90, Training loss 0.9819388577852712\n",
            "2023-12-13 02:23:08.297097 Epoch 92, Training loss 0.9787794472006581\n",
            "2023-12-13 02:23:48.291563 Epoch 94, Training loss 0.9773516759390721\n",
            "2023-12-13 02:24:25.624782 Epoch 96, Training loss 0.976048629302198\n",
            "2023-12-13 02:24:57.193850 Epoch 98, Training loss 0.9697863073909984\n",
            "2023-12-13 02:25:27.267275 Epoch 100, Training loss 0.9711209215471507\n",
            "2023-12-13 02:25:58.144541 Epoch 102, Training loss 0.9686356814925933\n",
            "2023-12-13 02:26:37.401630 Epoch 104, Training loss 0.9686493751642954\n",
            "2023-12-13 02:27:20.975582 Epoch 106, Training loss 0.9647699912338306\n",
            "2023-12-13 02:27:53.665688 Epoch 108, Training loss 0.9636236676169784\n",
            "2023-12-13 02:28:24.120156 Epoch 110, Training loss 0.9612402268840224\n",
            "2023-12-13 02:28:55.245392 Epoch 112, Training loss 0.9606205183831628\n",
            "2023-12-13 02:29:25.448251 Epoch 114, Training loss 0.9579403560484767\n",
            "2023-12-13 02:29:57.381833 Epoch 116, Training loss 0.9555173531517653\n",
            "2023-12-13 02:30:28.608726 Epoch 118, Training loss 0.9535850090596377\n",
            "2023-12-13 02:31:01.066752 Epoch 120, Training loss 0.9541207976505884\n",
            "2023-12-13 02:31:31.481120 Epoch 122, Training loss 0.9513152183016853\n",
            "2023-12-13 02:32:02.622282 Epoch 124, Training loss 0.9504487509160395\n",
            "2023-12-13 02:32:33.112110 Epoch 126, Training loss 0.9496010240843838\n",
            "2023-12-13 02:33:03.304654 Epoch 128, Training loss 0.9458811508725061\n",
            "2023-12-13 02:33:33.756335 Epoch 130, Training loss 0.9456738868485326\n",
            "2023-12-13 02:34:04.091851 Epoch 132, Training loss 0.9434349334148495\n",
            "2023-12-13 02:34:34.931624 Epoch 134, Training loss 0.9429770353657511\n",
            "2023-12-13 02:35:05.726050 Epoch 136, Training loss 0.9406395740521228\n",
            "2023-12-13 02:35:35.953674 Epoch 138, Training loss 0.9388583712565625\n",
            "2023-12-13 02:36:06.362193 Epoch 140, Training loss 0.940188133198282\n",
            "2023-12-13 02:36:36.607668 Epoch 142, Training loss 0.9371533547825825\n",
            "2023-12-13 02:37:07.820314 Epoch 144, Training loss 0.9365599341404712\n",
            "2023-12-13 02:37:38.303603 Epoch 146, Training loss 0.9334432199178144\n",
            "2023-12-13 02:38:08.537200 Epoch 148, Training loss 0.9331012154784044\n",
            "2023-12-13 02:38:38.643328 Epoch 150, Training loss 0.9324361755872321\n",
            "2023-12-13 02:39:08.915514 Epoch 152, Training loss 0.9311044811440246\n",
            "2023-12-13 02:39:39.638263 Epoch 154, Training loss 0.9314157896487\n",
            "2023-12-13 02:40:10.469586 Epoch 156, Training loss 0.9295521575928954\n",
            "2023-12-13 02:40:40.877562 Epoch 158, Training loss 0.9270947476482148\n",
            "2023-12-13 02:41:11.343830 Epoch 160, Training loss 0.9268501510705485\n",
            "2023-12-13 02:41:41.572787 Epoch 162, Training loss 0.926881563404332\n",
            "2023-12-13 02:42:12.434997 Epoch 164, Training loss 0.9254356213771474\n",
            "2023-12-13 02:42:43.331038 Epoch 166, Training loss 0.9248764945570466\n",
            "2023-12-13 02:43:13.578758 Epoch 168, Training loss 0.9211967957522863\n",
            "2023-12-13 02:43:43.857525 Epoch 170, Training loss 0.9214298177863021\n",
            "2023-12-13 02:44:14.322223 Epoch 172, Training loss 0.921768394806196\n",
            "2023-12-13 02:44:44.843234 Epoch 174, Training loss 0.9206090547392131\n",
            "2023-12-13 02:45:15.836490 Epoch 176, Training loss 0.918242136230859\n",
            "2023-12-13 02:45:45.949966 Epoch 178, Training loss 0.9179040580759268\n",
            "2023-12-13 02:46:16.112938 Epoch 180, Training loss 0.9159255350184867\n",
            "2023-12-13 02:46:46.050809 Epoch 182, Training loss 0.9155894349450651\n",
            "2023-12-13 02:47:16.212178 Epoch 184, Training loss 0.9171493420058199\n",
            "2023-12-13 02:47:47.321721 Epoch 186, Training loss 0.9155316889438483\n",
            "2023-12-13 02:48:17.762468 Epoch 188, Training loss 0.9132848153333835\n",
            "2023-12-13 02:48:47.847173 Epoch 190, Training loss 0.9126095813710976\n",
            "2023-12-13 02:49:17.926999 Epoch 192, Training loss 0.9133397923101245\n",
            "2023-12-13 02:49:47.909338 Epoch 194, Training loss 0.9117627073736752\n",
            "2023-12-13 02:50:18.737153 Epoch 196, Training loss 0.9096852579080236\n",
            "2023-12-13 02:50:49.215814 Epoch 198, Training loss 0.9094455002823754\n",
            "2023-12-13 02:51:19.330545 Epoch 200, Training loss 0.9084271663595038\n",
            "2023-12-13 02:51:49.530767 Epoch 202, Training loss 0.9073897645906415\n",
            "2023-12-13 02:52:19.384712 Epoch 204, Training loss 0.9086921654089027\n",
            "2023-12-13 02:52:49.662031 Epoch 206, Training loss 0.9078335016584762\n",
            "2023-12-13 02:53:20.706631 Epoch 208, Training loss 0.9063449844222544\n",
            "2023-12-13 02:53:50.818935 Epoch 210, Training loss 0.905834050968175\n",
            "2023-12-13 02:54:20.593205 Epoch 212, Training loss 0.9049688184352787\n",
            "2023-12-13 02:54:47.729393 Epoch 214, Training loss 0.9041280187761692\n",
            "2023-12-13 02:55:14.830844 Epoch 216, Training loss 0.9047222035315335\n",
            "2023-12-13 02:55:41.988404 Epoch 218, Training loss 0.9029671394306681\n",
            "2023-12-13 02:56:09.391143 Epoch 220, Training loss 0.9028866640899492\n",
            "2023-12-13 02:56:36.535676 Epoch 222, Training loss 0.9011293644338008\n",
            "2023-12-13 02:57:03.617586 Epoch 224, Training loss 0.9000922821824203\n",
            "2023-12-13 02:57:30.455769 Epoch 226, Training loss 0.90052894138924\n",
            "2023-12-13 02:57:57.369039 Epoch 228, Training loss 0.8977465049537552\n",
            "2023-12-13 02:58:24.486753 Epoch 230, Training loss 0.9001340116838665\n",
            "2023-12-13 02:58:51.576794 Epoch 232, Training loss 0.898897972405719\n",
            "2023-12-13 02:59:18.849543 Epoch 234, Training loss 0.896470859532466\n",
            "2023-12-13 02:59:46.120141 Epoch 236, Training loss 0.8966977580276596\n",
            "2023-12-13 03:00:13.125394 Epoch 238, Training loss 0.8958936175117103\n",
            "2023-12-13 03:00:41.231815 Epoch 240, Training loss 0.8961248979010545\n",
            "2023-12-13 03:01:09.386986 Epoch 242, Training loss 0.8950302619915789\n",
            "2023-12-13 03:01:37.196120 Epoch 244, Training loss 0.8967683280596648\n",
            "2023-12-13 03:02:04.570094 Epoch 246, Training loss 0.8931419586434084\n",
            "2023-12-13 03:02:32.052338 Epoch 248, Training loss 0.8931875847794516\n",
            "2023-12-13 03:02:59.504022 Epoch 250, Training loss 0.8939148095791297\n",
            "2023-12-13 03:03:27.107101 Epoch 252, Training loss 0.8930414822857703\n",
            "2023-12-13 03:03:54.596813 Epoch 254, Training loss 0.8934311002416684\n",
            "2023-12-13 03:04:21.748583 Epoch 256, Training loss 0.8919112002453231\n",
            "2023-12-13 03:04:49.217998 Epoch 258, Training loss 0.8913291956457641\n",
            "2023-12-13 03:05:16.582767 Epoch 260, Training loss 0.889785592208433\n",
            "2023-12-13 03:05:43.848934 Epoch 262, Training loss 0.8889671779044753\n",
            "2023-12-13 03:06:12.663691 Epoch 264, Training loss 0.8920814125129329\n",
            "2023-12-13 03:06:40.003606 Epoch 266, Training loss 0.8881299295236388\n",
            "2023-12-13 03:07:07.266550 Epoch 268, Training loss 0.8861419047083696\n",
            "2023-12-13 03:07:34.721520 Epoch 270, Training loss 0.8879268971245612\n",
            "2023-12-13 03:08:02.136913 Epoch 272, Training loss 0.8861237139729283\n",
            "2023-12-13 03:08:29.483464 Epoch 274, Training loss 0.8867222713616193\n",
            "2023-12-13 03:08:57.014385 Epoch 276, Training loss 0.8854958146734311\n",
            "2023-12-13 03:09:24.302870 Epoch 278, Training loss 0.8870160516418154\n",
            "2023-12-13 03:09:51.467951 Epoch 280, Training loss 0.8848764891057368\n",
            "2023-12-13 03:10:18.515098 Epoch 282, Training loss 0.8847859363879085\n",
            "2023-12-13 03:10:45.706538 Epoch 284, Training loss 0.8836326820161337\n",
            "2023-12-13 03:11:12.816610 Epoch 286, Training loss 0.8832297419648036\n",
            "2023-12-13 03:11:40.150144 Epoch 288, Training loss 0.8823670920965921\n",
            "2023-12-13 03:12:07.506461 Epoch 290, Training loss 0.8830102151617065\n",
            "2023-12-13 03:12:34.653452 Epoch 292, Training loss 0.8827567619969473\n",
            "2023-12-13 03:13:02.036570 Epoch 294, Training loss 0.8816874133199072\n",
            "2023-12-13 03:13:29.750428 Epoch 296, Training loss 0.8826848773090431\n",
            "2023-12-13 03:13:57.112059 Epoch 298, Training loss 0.8809415912994033\n",
            "2023-12-13 03:14:24.485648 Epoch 300, Training loss 0.8792145011751243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valLoader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "accuracy2, predictions2, expected_labels2 = validate(model2, trainLoader, valLoader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXlQUdzU_Fju",
        "outputId": "e394c5e5-00cd-42ec-f147-d6240ad622ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.64\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
        "precision2 = precision_score(predictions2, expected_labels2, average='macro')\n",
        "recall2 = recall_score(predictions2, expected_labels2, average='macro')\n",
        "cnfMatrix2 = confusion_matrix(predictions2, expected_labels2)\n",
        "print(cnfMatrix2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBAfPtS-_UgU",
        "outputId": "1bd9e661-eaf7-41ac-cb9d-df83017cb0cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4373  102  603  179  231   71   76   83 1052  114]\n",
            " [ 288 4730   91  106   55   55   62   80  588  426]\n",
            " [ 186   21 2271  203  226  157  175   56   48    5]\n",
            " [ 103   30  436 2428  358  903  394  193  117   47]\n",
            " [ 137   26  699  371 3351  264  304  199   82   28]\n",
            " [  56   17  619 1429  276 3551  180  450   42   34]\n",
            " [  38   30  572  586  476  194 4572   64   53   29]\n",
            " [ 179   48  526  399  884  642   99 4596   54  140]\n",
            " [ 204   30   58   69   37   16   36   17 3363   43]\n",
            " [ 436  966  125  230  106  147  102  262  601 5134]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions2, expected_labels2, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qrWdWJRNQgM",
        "outputId": "825181cb-6513-4841-c710-fd5db8fd4d0f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.73      0.64      0.68      6884\n",
            "  automobile       0.79      0.73      0.76      6481\n",
            "        bird       0.38      0.68      0.49      3348\n",
            "         cat       0.40      0.48      0.44      5009\n",
            "        deer       0.56      0.61      0.58      5461\n",
            "         dog       0.59      0.53      0.56      6654\n",
            "        frog       0.76      0.69      0.72      6614\n",
            "       horse       0.77      0.61      0.68      7567\n",
            "        ship       0.56      0.87      0.68      3873\n",
            "       truck       0.86      0.63      0.73      8109\n",
            "\n",
            "    accuracy                           0.64     60000\n",
            "   macro avg       0.64      0.65      0.63     60000\n",
            "weighted avg       0.67      0.64      0.65     60000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P2Uw2JnfL5nG"
      }
    }
  ]
}